# OPEA Local Configuration
# Based on ChatQnA examples

# General settings
log_level: info
port: 8000

# Model settings
model:
  # Change this path to match your local model location
  path: "./models/Meta-Llama-3.2-3B-Instruct-Q6_K_L.gguf"
  type: "llm"
  # Adjust based on your GPU memory
  gpu_memory: "auto"  
  # Set to true to use CUDA for NVIDIA GPUs
  use_cuda: true  

# Generation settings
generation:
  max_new_tokens: 1024
  temperature: 0.7
  top_p: 0.9
  top_k: 40
  repetition_penalty: 1.1
  
# API settings
api:
  # Set to true for production use
  enable_auth: false  
  # Format of the chat API (openai compatible)
  format: "openai"
  
# System settings
system:
  # Adjust based on your hardware
  num_workers: 1  
  # Path for temporary storage
  cache_dir: "./opea_cache"  
  # Path for logs
  log_dir: "./opea_logs" 