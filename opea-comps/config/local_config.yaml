# OPEA Local Configuration
# Based on ChatQnA examples

# Logging configuration
logging:
  level: info
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  datefmt: "%Y-%m-%d %H:%M:%S"

# Server configuration
server:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  timeout: 120
  backend: "uvicorn"

# Model configuration
model:
  # Change this path to match your local model location
  path: "./models/Meta-Llama-3.2-3B-Instruct-Q6_K_L.gguf"
  type: "llm"
  name: "Meta-Llama-3.2-3B-Instruct"
  framework: "llama.cpp"
  # GPU configuration
  device:
    type: "cuda"
    id: 0
  # Memory settings
  memory:
    gpu_memory: "auto"
    cpu_memory: 2000  # MB

# Inference settings
generation:
  max_new_tokens: 1024
  temperature: 0.7
  top_p: 0.9
  top_k: 40
  repetition_penalty: 1.1
  do_sample: true
  
# API settings
api:
  # Set to true for production use
  enable_auth: false
  # API keys (when auth is enabled)
  api_keys: []
  # Format of the chat API
  format: "openai"
  # CORS settings
  cors:
    allow_origins: ["*"]
    allow_methods: ["*"]
    allow_headers: ["*"]
  
# System settings
system:
  # Path for temporary storage
  cache_dir: "./opea_cache"
  # Path for logs
  log_dir: "./opea_logs"
  # Cleanup interval in seconds
  cleanup_interval: 3600
  # Max cache size in MB
  max_cache_size: 1000 