# Server configuration
PORT=3000

# LLM API configuration
LLM_API_KEY=your_api_key_here
LLM_API_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4

# To use a local LLM provider (like LM Studio, Ollama, etc.), use settings like these:
#LLM_API_KEY=not-needed  # Many local providers don't need an API key
#LLM_API_BASE_URL=http://http://10.0.0.47:1234/v1  # Local model API endpoint
#LLM_MODEL=mistral3-24b  # Name of the local model

# Provider-specific configuration
# Path to the completions endpoint - varies by provider
# OpenAI: chat/completions
# Ollama: chat/completions or generate
# Others may use different paths
LLM_ENDPOINT_PATH=chat/completions

# Request timeout in milliseconds
REQUEST_TIMEOUT_MS=30000

# Debug settings
# Set to 'true' to see detailed request/response logs
DEBUG_MODE=true

# CORS configuration (optional)
# CORS_ORIGIN=http://localhost:8501 