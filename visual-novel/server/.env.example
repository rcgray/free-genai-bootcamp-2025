# Server configuration
PORT=3011

# LLM API configuration
LLM_API_KEY=your_api_key_here
LLM_API_BASE_URL=https://api.openai.com/v1
LLM_MODEL=o3-mini

# Current Pricing per 1M tokens (as of 2025-03-20)
# gpt-4o:       $2.50  # full response, all content, ~47s
# o1-mini:      $1.10  # ok response, cultural/learning tips unpredictable, ~43s
# o3-mini:      $1.10  # ok response, cultural/learning tips unpredictable, ~40s
# gpt-4o-mini:  $0.15  # ok response, cultural/learning tips unpredictable, ~46s

# To use a local LLM provider (like LM Studio, Ollama, etc.), use settings like these:
#LLM_API_KEY=not-needed  # Many local providers don't need an API key
#LLM_API_BASE_URL=http://http://10.0.0.47:1234/v1  # Local model API endpoint
#LLM_MODEL=mistral3-24b  # Name of the local model

# Provider-specific configuration
# Path to the completions endpoint - varies by provider
# OpenAI: chat/completions
# Ollama: chat/completions or generate
# Others may use different paths
LLM_ENDPOINT_PATH=chat/completions

# Request timeout in milliseconds
REQUEST_TIMEOUT_MS=60000

# Debug settings
# Set to 'true' to see detailed request/response logs
DEBUG_MODE=true

# CORS configuration (optional)
# CORS_ORIGIN=http://localhost:8501 